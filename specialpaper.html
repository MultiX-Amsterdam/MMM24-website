<!doctype html>
<html>

<head>
  <title>MMM2024</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/footer.js"></script>
  <style>
    .menu-submit {
      color: rgb(0, 0, 0) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="banner" style="background: url('img/feature.jpeg') no-repeat center; background-size: cover; height: 450px;">
      <div class="banner-table flex-column" style="background-color: rgba(0, 0, 0, 0.5);">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Call for Special Session Papers</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <ul>
              <li><a href="#s1">MDRE: Multimedia Datasets for Repeatable Experimentation</a></li>
              <li><a href="#s2">MOMST: Multi-Object Multi-Sensor Tracking</a></li>
              <li><a href="#s3">MARGeM: Multimodal Analytics and Retrieval of Georeferenced Multimedia</a></li>
              <li><a href="#s4">ICDAR: Intelligent Cross-Data Analysis and Retrieval</a></li>
              <li><a href="#s5">XR-MACCI: eXtended Reality and Multimedia - Advancing Content Creation and Interaction</a></li>
              <li><a href="#s6">FMM: Foundation Models for Multimedia</a></li>
              <li><a href="#s7">MULTICON: Towards Multimedia and Multimodality in Conversational Systems</a></li>
              <li><a href="#s8">CultMM: Cultural AI in Multimedia</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s1">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">MDRE: Multimedia Datasets for Repeatable Experimentation</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Information retrieval and multimedia content access has a long history of comparative evaluation and many of the advances in the area over the past decade can be attributed to the availability of open datasets that support comparative and repeatable experimentation.
              Sharing data and code to allow other researchers to replicate research results is needed in the multimedia modeling field and this will help to improve the performance of systems and the reproducibility of papers published.
            </p>
            <p class="text">
              This multimedia dataset track will be an opportunity for researchers and practitioners to make their work permanently available and citable in a single forum, as well as to increase the public awareness of their considerable efforts.
            </p>
            <p class="text">
              Researchers within the multimedia community will be encouraged to submit their datasets, or papers related to dataset-generation to this track.
              Authors of dataset papers are asked to provide a paper describing its motivation, design, and usage, a brief summary of the experiments performed to date on the dataset, as well as discussing the way it can be useful to the community.
              The benefits for authors who successfully submit are:
            </p>
            <ul>
              <li>Accepted contributions will be included in the conference proceedings.</li>
              <li>Accepted contributions will be listed in a recognised index of multimedia datasets, thereby increasing their visibility.</li>
              <li>Authors of accepted contributions will be invited to present their dataset as part of the special session programme at MMM2024.</li>
            </ul>
            <p class="text">
              Regarding the submission of a dataset, the authors should make it available by providing a URL for download, as mentioned above, and agree to the link being maintained on an MMM datasets dedicated site.
              All datasets must be licensed in such a manner that it can be legally and freely used with all appropriate ethical and access approvals completed.
              Authors are encouraged to prepare appropriate and helpful documentation to accompany the dataset, including examples of how it can be used by the community, examples of successful usage and restrictions on usage.
            </p>
            <p class="text">
              We may additionally accept position papers of high quality, which we believe can significantly impact multimedia datasets in the future, by addressing various aspects of dataset creation methodologies.
              We will prefer position papers that are backed up by recent results, which could be already published or appear first in the MDRE submission.
            </p>
            <p class="text">
              Authors do not need to anonymize their submission due to the inherent difficulty of doing so for open datasets.
            </p>
            <h2>Organizers</h2>
            <hr>
            <ul>
              <li>Klaus Schöffmann, Klagenfurt University, Austria</li>
              <li>Björn Þór Jónsson, Reykjavik University, Iceland</li>
              <li>Cathal Gurrin, Dublin City University, Ireland</li>
              <li>Duc-Tien Dang-Nguyen, University of Bergen, Norway</li>
              <li>Liting Zhou, Dublin City University, Ireland</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s2">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">MOMST: Multi-Object Multi-Sensor Tracking</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Multi-object multi-sensor tracking (MOMST) is a complex problem in computer vision and machine learning, involving the simultaneous tracking of multiple objects using data from multiple sensors.
              MOMST is essential in many applications, such as surveillance systems, autonomous vehicles, and robotics.
            </p>
            <p class="text">
              MOMST algorithms typically use a combination of sensor fusion and data association techniques to estimate the state of each object over time.
              Sensor fusion involves combining data from multiple sensors to obtain a more accurate and complete representation of the objects being tracked (as an enhancement of <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11878/118780I/Multi-object-tracking-a-survey/10.1117/12.2602901.short?SSO=1">MOT challenges</a>).
              Data association involves matching sensor measurements to tracks, which can be challenging in scenarios where there are occlusions, clutter, or sensor failures.
              One popular approach to MOMST is multiple hypothesis tracking (<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-ipr.2017.1244">MHT</a>), which maintains multiple possible tracks for each object and updates the probabilities of each track as new sensor measurements become available.
              Other approaches include the use of deep learning techniques, such as object detection and tracking with convolutional neural networks (CNNs) and recurrent neural networks (RNNs).
            </p>
            <p class="text">
              MOMST is a challenging problem due to the complexity of real-world environments and the limitations of sensor technology.
              Sensor measurements can be noisy, incomplete, and prone to errors, and objects can move in unpredictable ways.
              MOMST algorithms must be robust to handle these challenges and produce accurate and reliable results.
              Despite these challenges, MOMST has many important applications.
              In autonomous vehicles, MOMST is used to track other vehicles, pedestrians, and obstacles in the environment to ensure safe and efficient navigation.
              In robotics, MOMST is used to track objects in dynamic environments, such as warehouses or manufacturing facilities.
              In surveillance systems, MOMST is used to track people and vehicles in public spaces to prevent crime and enhance public safety.
            </p>
            <p class="text">
              In conclusion, MOMST is an important and challenging problem in computer vision and machine learning.
              Advances in sensor technology and algorithmic techniques have made significant progress in recent years, but there is still much work to be done to improve the accuracy and reliability of MOMST algorithms in complex real-world scenarios.
            </p>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Mario Döller, University of Applied Sciences Kufstein Tirol, Austria</li>
              <li>Ruben Tous, University Politecnica de Catalunya (UPC), Spain</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s3">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">MARGeM: Multimodal Analytics and Retrieval of Georeferenced Multimedia</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Georeferenced multimedia data, such as satellite images and videos, are a key resource for researchers and practitioners in fields such as Earth Observation, urban computing, and lifelogging.
              However, this type of data is often highly heterogeneous, distributed, and semantically fragmented, which presents significant challenges for effective analysis and retrieval.
              The emergence of deep learning and multimodal analytics provides an opportunity to overcome these challenges and unlock the full potential of georeferenced multimedia data.
              By leveraging the strengths of different data modalities, researchers can enhance the value of these datasets and gain insights that were previously impossible to obtain.
            </p>
            <p class="text">
              In this context, this special session invites papers in the area of multimodal analytics and retrieval that leverage the importance of spatial information when combined with other data modalities, the value of the original data.
              Despite the popularity of research areas like cross-modal retrieval, image captioning, image generation, and visual question answering (VQA) in multimedia, however, their potential has yet to be fully explored in the context of location-based services.
              It is crucial to deploy interpretable machine learning techniques to unlock the knowledge that is hidden in multimodal data with geospatial information, given that many of the problems studied in multimedia are NP-hard and require approximation to determine the degree of computing power they can provide.
            </p>
            <p class="text">
              We believe that the MARGeM special session can serve as a joint venue for the different communities working on georeferenced data and its many applications, and thus propel the cross-fertilisation of ideas, methods and software between the communities.
            </p>
            <p class="text">
              This special session includes presentation of novel research within the following domains:
            </p>
            <ul>
              <li>lifelog computing</li>
              <li>urban computing</li>
              <li>satellite computing and earth observation</li>
            </ul>
            <p class="text">
              Within these domains, the topics of interest include (but are not restricted to):
            </p>
            <ul>
              <li>Multimodal analytics and retrieval techniques for georeferenced multimedia data</li>
              <li>Deep learning and neural networks for interpretability, understanding, and explainability in artificial intelligence applied to georeferenced multimedia data</li>
              <li>Cross-modal retrieval, image captioning, image generation, and visual question answering for location-based services</li>
              <li>Satellite image analysis and retrieval Semantically-aware approaches for handling highly heterogeneous, distributed and semantically fragmented georeferenced multimedia data</li>
              <li>Interpretable machine learning techniques for unlocking hidden knowledge in big georeferenced multimedia data</li>
              <li>Digital Twins based on georeferenced multimedia</li>
              <li>Applications of georeferenced multimedia data in urban and lifelog computing</li>
              <li>Big data analytics and visualization on GIS platforms for georeferenced multimedia data.</li>
            </ul>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Maria Pegia, Centre for Research and Technology Hellas, Information Technologies Institute, Greece</li>
              <li>Ioannis Papoutsis, National Observatory of Athens, Greece</li>
              <li>Ilias Gialampoukidis, Centre for Research and Technology Hellas, Information Technologies Institute, Greece</li>
              <li>Björn Þór Jónsson Professor, Department of Computer Science, Reykjavik University, Iceland</li>
              <li>Stefanos Vrochidis, Centre for Research and Technology Hellas, Information Technologies Institute</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s4">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">ICDAR: Intelligent Cross-Data Analysis and Retrieval</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Data has become a critical component of human life in the digital age, where it can be collected from various sources and in real-time, providing valuable insights into our living environment.
              However, these data sources only represent a small piece of the larger puzzle of life.
              Therefore, the ability to collect and analyze data across multiple domains, modalities, and platforms is crucial to solving this puzzle faster.
              Recent research has focused on multimodal data analytics, but there is a lack of investigation into cross-data analysis and retrieval.
              This research direction includes cross-modal data, cross-domain, and cross-platform data analysis and retrieval.
              For example, cross-modal retrieval systems use a textual query to look for images, while air quality index can be predicted using lifelogging images, and daily exercises and meals can help predict sleeping quality.
            </p>
            <p class="text">
              To promote intelligent cross-data analytics and retrieval research and create a smarter, sustainable society, we invite submissions to a special article collection on "Intelligent Cross-Data Analysis and Retrieval."
              We welcome submissions from diverse research domains and disciplines, including well-being, disaster prevention and mitigation, mobility, climate change, tourism, healthcare, and food computing.
              Join us in exploring the exciting field of cross-data analysis and retrieval!
            </p>
            <p class="text">
              This Research Topic welcomes submissions from diverse research domains and disciplines such as well-being, disaster prevention and mitigation, mobility, climate change, tourism, healthcare, and food computing.
              Example topics of interest include, but are not limited to:
            </p>
            <ul>
              <li>Event-based cross-data retrieval</li>
              <li>Data mining and AI technology</li>
              <li>Complex event processing for linking sensors data from individuals, regions to broad areas dynamically</li>
              <li>Transfer Learning and Transformers</li>
              <li>Hypotheses development of the associations within the heterogeneous data</li>
              <li>Realization of a prosperous and independent region in which people and nature coexist</li>
              <li>Applications leveraging intelligent cross-data analysis for a particular domain</li>
              <li>Cross-datasets for repeatable experimentation</li>
              <li>Federated Analytics and Federated Learning for cross-data</li>
              <li>Privacy-public data collaboration</li>
              <li>Integration of diverse multimodal data</li>
            </ul>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Minh-Son Dao, National Institute of Information and Communications Technology, Japan</li>
              <li>Michael Alexander Riegler, Simula Metropolitan Center for Digital Engineering, Norway</li>
              <li>Duc Tien Dang Nguyen, University of Bergen, Norway</li>
              <li>Thanh-Binh Nguyen, University of Science, Vietnam National University in HCM City</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s5">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">XR-MACCI: eXtended Reality and Multimedia - Advancing Content Creation and Interaction</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Extended Reality and Multimedia: Advancing Content Creation and Interaction (XR-MACCI) special session at the Multimedia Modelling 2024 conference invites researchers, industry experts, and enthusiasts to explore the latest advancements in extended reality (XR) and multimedia technologies.
              This session will focus on the development and integration of XR solutions with multimedia analysis, retrieval and processing methods, emphasizing seamless and interactive experiences that transform the way we live, work, and interact with our surroundings.
            </p>
            <p class="text">
              The XR-MACCI 2024 special session will address the following key topics:
            </p>
            <ul>
              <li>Next-Generation XR Technologies: Exploring cutting-edge solutions in virtual reality (VR), augmented reality (AR), and mixed reality (MR) that push the boundaries of immersive multimedia experiences.</li>
              <li>Real-time 3D Modeling and Rendering: Investigating innovative techniques for creating realistic and dynamic 3D models and environments, enabling high-quality visuals and interactions in XR applications.</li>
              <li>Adaptive and Interactive Content Delivery: Developing methods for optimizing and personalizing multimedia content based on user preferences, context, and device capabilities, ensuring a seamless XR experience.</li>
              <li>AI for XR Content Creation: Utilizing artificial intelligence and machine learning for content analysis, understanding and retrieval to facilitate XR content generation.</li>
              <li>AI-Driven Multimedia and XR Integration: Utilizing artificial intelligence and machine learning to enhance recognition and manipulation in XR environments, leading to more intuitive and engaging experiences.</li>
              <li>Multisensory Interfaces and Wearable Technologies: Investigating the latest advancements in haptic feedback, gesture recognition, and sensory input/output devices that facilitate natural and immersive interactions with XR and multimedia content.</li>
            </ul>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Claudio Gennaro, Information Science and Technologies Institute, National Research Council, Italy</li>
              <li>Sotiris Diplaris, Information Technologies Institute, Centre for Research and Technology Hellas, Greece</li>
              <li>Stefanos Vrochidis,  Information Technologies Institute, Centre for Research and Technology Hellas, Greece</li>
              <li>Heiko Schuldt, University of Basel, Switzerland</li>
              <li>Werner Bailer, Joanneum Research, Austria</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s6">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">FMM: Foundation Models for Multimedia</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Foundations models (FMs), currently in the form of large language models (LLMs) and large vision language models (LVLMs), are reshaping the way in which multimedia content is generated, analyzed, interpreted, and retrieved.
              While the current cyberspace remains dominated by user generated content (UGC), they are likely to be outnumbered by artificial intelligence generated content (AIGC) in the near future, with multimodal FMs as the driving force for such a seismic change.
            </p>
            <p class="text">
              Prompt engineering (PE) techniques are being actively developed for harnessing FMs for open-ended multimedia content recognition and interpretation.
              Also, it can be largely anticipated that in contrast to the current video search engine which answers a user's query by ranking existing videos in terms of their relevance with respect to the query, a next-generation video search engine will work in a ranking-and-generation manner.
            </p>
            <p class="text">
              Recognizing that FMs are essential to the future of multimedia computing, the remaining question is then how shall the multimedia community meet the new future?
              Given that both the development of FMs and their application on multimedia are still early-stage, we believe a special session on "Foundation Models for Multimedia" (FMM) will be a very timely reflection of the latest development on the topic and hopefully provide a partial answer to the question.
            </p>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Xirong Li, Renmin University of China, China</li>
              <li>Zhineng Chen, Fudan University, China </li>
              <li>Xing Xu, University of Electronic Science and Technology of China </li>
              <li>Symeon (Akis) Papadopoulos, Centre for Research and Technology Hellas, Greece </li>
              <li>Jing Liu, Chinese Academy of Sciences</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s7">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">MULTICON: Towards Multimedia and Multimodality in Conversational Systems</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              The increasing interest in advanced and human-like conversational systems, along with the rise of various digital communications channels such as social media, intelligent agents, and chatbots, leads to a pressing need to enhance their capabilities.
              However, traditional chatbots and virtual assistants have intrinsic limitations in their ability to engage users in natural and intuitive conversations, especially when involving different sources of multimedia and multimodal information.
              Incorporating multimedia and multimodality, such as visual and audio cues, into such conversational systems can lead to a better understanding of users' needs and intentions and provide a significantly improved user experience.
            </p>
            <p class="text">
              At the same time, the recent advancements in Large Language Models (LLMs), such as Open AI's ChatGPT, Google's Bard, and Stanford's Alpaca, have further opened up new opportunities to improve the performance of these systems, generating more natural and coherent responses.
              The integration of multimodality in conversational systems paired with the significant capabilities of LLMs is also an area of increasing research interest since it allows for more natural, intuitive, and engaging conversations.
            </p>
            <p class="text">
              This special session aims to present the most recent works and applications for addressing the challenges and opportunities in developing multimedia and multimodality-enabled conversational systems and chatbots.
              Indicative domains of application include healthcare, education, immigration, customer service, finance and others.
            </p>
            <p class="text">
              Topics of interest include, but are not limited to the following:
            </p>
            <ul>
              <li>Multimodal and multimedia open-ended chatbots</li>
              <li>Multimodal and multimedia task-oriented chatbots</li>
              <li>Novel architectures for multimodal chatbots and conversational systems</li>
              <li>Context-aware technologies for multimodal chatbots with Transformers and Large Language Models (LLMs)</li>
              <li>Multimodal query processing and understanding in conversational systems</li>
              <li>Integration of multimodal knowledge graphs in conversational systems</li>
              <li>Knowledge distillation techniques for transferring knowledge from LLMs to smaller models for multimodal chatbots</li>
              <li>Chatbots and conversational systems in healthcare and the medical domain</li>
              <li>Multimodal data fusion for various applications, including education, immigration, customer service, etc.</li>
              <li>Evaluation of multimodal conversational systems, including evaluation metrics for measuring the coherence and fluency of the generated responses</li>
              <li>Evaluation of the user experience, including evaluation metrics for measuring user satisfaction, engagement, and system usability</li>
            </ul>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Thanassis Mavropoulos, Centre for Research and Technology Hellas, Information Technologies Institute, Greece</li>
              <li>Georgios Meditskos,School of Informatics, Aristotle University of Thessaloniki, Greece</li>
              <li>Stefanos Vrochidis, Centre for Research and Technology Hellas, Information Technologies Institute</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="banner" id="s8">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">CultMM: Cultural AI in Multimedia</h2>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              Cultural AI focuses on developing systems that can deal with the complexities of human culture, thereby improving applications to cultural data and enhancing AI systems' ability to deal with cultural complexities.
              An increasingly clear insight is that many of the complexities of culture are highly contextualised which often expresses itself in a multimodal manner.
              For instance, analysis of widely published iconic images (e.g., napalm girl, migrant mother) includes the image itself, the contexts in which it has been published, and how it has been received [<a href="https://academic.oup.com/dsh/article/37/4/1316/6534688">1</a>].
              Similarly, the prominence of linked data representations of cultural data are opening up new possibilities for enriching datasets and visual content analysis [<a href="https://link.springer.com/chapter/10.1007/978-3-030-15200-0_18">2</a>].
              As such, we argue that a cultural perspective is inherently a multimedia perspective, and whilst applications of multimedia systems to cultural data have always been at home in the multimedia community [<a href="https://dl.acm.org/doi/abs/10.1145/2072529.2072531">3</a>,<a href="https://dl.acm.org/doi/abs/10.1145/3273024.3273032">4</a>], the complexities that come from analysing culture (e.g., [<a href="https://arxiv.org/abs/2211.07460">5</a>]) have insufficiently been foregrounded.
            </p>
            <p class="text">
              With this special session we aim to bring together experts from Cultural AI and Multimedia to discuss the challenges surrounding cultural data as well as the complexities of human culture.
              Additionally, we aim to demonstrate that culture is more than an aesthetically pleasing testbed for multimedia systems, and that culture offers new challenges that require multimedia solutions.
              In addition to technical papers we also explicitly invite high-quality position papers on related topics to cultural AI, or that highlight cultural challenges which require multimedia solutions.
              We will prioritise position papers that are supported by recent published results or preliminary findings that are being described for the first time in the submitted paper.
            </p>
            <h2 class="add-top-margin-small">Organizers</h2>
            <hr>
            <ul>
              <li>Nanne van Noord, University of Amsterdam, Netherlands</li>
              <li>Melvin Wevers, University of Amsterdam, Netherlands</li>
              <li>Stuart James, Italian Institute of Technology & UCL Centre for Digital Humanities, Italy</li>
              <li>Cynthia Liem, TU Delft, Netherlands</li>
              <li>Victor de Boer, Vrije Universiteit Amsterdam, Netherlands</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="footer-container"></div>
</body>

</html>